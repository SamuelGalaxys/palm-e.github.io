
<!DOCTYPE html>
<html>

<head lang="en">
    <meta charset="UTF-8">
    <meta http-equiv="x-ua-compatible" content="ie=edge">

    <title>PaLM-E: An Embodied Multimodal Language Model</title>

    <meta name="description" content="PaLM-E: An Embodied Multimodal Language Model">
    <meta name="viewport" content="width=device-width, initial-scale=1">

    <!-- <base href="/"> -->

        <!--FACEBOOK-->
    <meta property="og:image" content="https://palm-e.github.io/img/rt1-teaser.jpeg">
    <meta property="og:image:type" content="image/jpeg">
    <meta property="og:image:width" content="682">
    <meta property="og:image:height" content="682">
    <meta property="og:type" content="website" />
    <meta property="og:url" content="https://palm-e.github.io/"/>
    <meta property="og:title" content="PaLM-E: An Embodied Multimodal Language Model" />
    <meta property="og:description" content="Project page for PaLM-E: An Embodied Multimodal Language Model." />

        <!--TWITTER-->
    <meta name="twitter:card" content="summary_large_image" />
    <meta name="twitter:title" content="PaLM-E: An Embodied Multimodal Language Model" />
    <meta name="twitter:description" content="Project page for PaLM-E: An Embodied Multimodal Language Model." />
    <meta name="twitter:image" content="https://palm-e.github.io/img/rt1-teaser.jpeg" />


<!--     <link rel="apple-touch-icon" href="apple-touch-icon.png"> -->
  <!-- <link rel="icon" type="image/png" href="img/seal_icon.png"> -->
    <!-- Place favicon.ico in the root directory -->

    <link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/bootstrap/3.3.5/css/bootstrap.min.css">
    <link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/font-awesome/4.4.0/css/font-awesome.min.css">
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/codemirror/5.8.0/codemirror.min.css">
    <link rel="stylesheet" href="css/app.css">

    <link rel="stylesheet" href="css/bootstrap.min.css">

    <script src="https://ajax.googleapis.com/ajax/libs/jquery/1.11.3/jquery.min.js"></script>
    <script src="https://maxcdn.bootstrapcdn.com/bootstrap/3.3.5/js/bootstrap.min.js"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/codemirror/5.8.0/codemirror.min.js"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/clipboard.js/1.5.3/clipboard.min.js"></script>
    
    <script src="js/app.js"></script>
    <script>

function myFunction(imgs) {
    // Get the expanded image
    var expandImg = document.getElementById("expandedImg");
    // Get the image text
    var imgText = document.getElementById("imgtext");
    var answer = document.getElementById("answer");

    // Use the same src in the expanded image as the image being clicked on from the grid
    expandImg.src = imgs.src;
    // Use the value of the alt attribute of the clickable image as text inside the expanded image
    var qa = imgs.alt.split("[sep]");
    imgText.innerHTML = qa[0];
    answer.innerHTML = "";
    // Show the container element (hidden with CSS)
    expandImg.parentElement.style.display = "block";
    typeWriter(qa[1], 0, qa[0]);
    }

function typeWriter(txt, i, q) {
    var imgText = document.getElementById("imgtext");
    if (imgText.innerHTML == q) {
    if (i < txt.length) {
        document.getElementById("answer").innerHTML += txt.charAt(i);
        i++;
        setTimeout(typeWriter, 20, txt, i, q);
    }
    }
}
</script>
<!-- Google tag (gtag.js) -->
<script async src="https://www.googletagmanager.com/gtag/js?id=G-52J0PM8XKV"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'G-52J0PM8XKV');
</script>
	
    <style>
        .nav-pills {
          position: relative;
          display: inline;
        }
        .imtip {
          position: absolute;
          top: 0;
          left: 0;
        }
    </style>
</head>

<body>
    <div class="container" id="main">
        <div class="row">
            <h2 class="col-md-12 text-center">
                <strong><font size="+6">PaLM-E: An Embodied Multimodal Language Model</font></strong> 

            </h2>
        </div>
        <div class="row">
            <div class="col-md-12 text-center">
                
                <ul class="list-inline">
                    <br>
                    <li>Danny Driess<sup>1,2</sup></li> <li>Fei Xia<sup>1</sup> </li> <li> Mehdi S. M. Sajjadi<sup>3</sup> </li> <li> Corey Lynch<sup>1</sup></li> <li>Aakanksha Chowdhery<sup>3</sup> </li>  <br>
                    <li> Brian Ichter<sup>1</sup></li> <li>  Ayzaan Wahid<sup>1</sup> </li> <li> Jonathan Tompson<sup>1</sup> </li> <li> Quan Vuong<sup>1</sup> </li> <li> Tianhe Yu<sup>1</sup> </li> <li> Wenlong Huang<sup>1</sup> </li> <br>
                    <li> Yevgen Chebotar<sup>1</sup> </li>  <li> Pierre Sermanet<sup>1</sup> </li> <li> Daniel Duckworth<sup>3</sup> </li> <li> Sergey Levine<sup>1</sup> </li> <li> Vincent Vanhoucke<sup>1</sup> </li>  <br>
                    <li> Karol Hausman<sup>1</sup> </li> <li> Marc Toussaint<sup>2</sup></li> <li> Klaus Greff<sup>3</sup> </li> <li> Andy Zeng<sup>1</sup> </li><li> Igor Mordatch<sup>3</sup> </li> <li> Pete Florence<sup>1</sup> </li>
                    <br><br>
                    <sup>1</sup><a href="http://g.co/robotics">
                        <img src="img/rng-logo.png" height="37px"> </a>
                        <sup>2</sup><a href="https://www.tu.berlin/en/">
                        <img src="img/tuberlin.png" height="32px"> </a> &nbsp;&nbsp;
                        <sup>3</sup><a href="https://research.google/teams/brain/">  
                        <img src="img/google-research-logo.png" height="25px"> </a> 
                        
                    </ul>

            </div>
        </div>


        <div class="row">
            <div class="col-md-4 col-md-offset-4 text-center">
                <ul class="nav nav-pills nav-justified">
                    <li>
                        <a href="#">
                        <img src="img/paper_thumb.png" height="60px">
                            <h4><strong>Paper</strong></h4>
                        </a>
                    </li>

                    <li>
                        <a href="#">
                        <img src="img/youtube_icon.png" height="60px">
                            <h4><strong>Video</strong></h4>
                        </a>
                    </li>
<!--                         <li>
                        <a href="http://ai.googleblog.com/2022/12/rt-1-robotics-transformer-for-real.html">
                        <image src="img/google-ai-blog-small.png" height="60px">
                            <h4><strong>Blogpost</strong></h4>
                        </a>
                    </li>
                     <li>
                        <a href="https://github.com/google-research/robotics_transformer">
                        <image src="img/github.png" height="60px">
                            <h4><strong>Code</strong></h4>
                        </a>                   
                    </li>  -->
                </ul>
            </div>
        </div>

        <div class="row">
            <div class="col-md-8 col-md-offset-2">
                <p style="text-align:center;">
        	    	<video id="v0" width="100%" playsinline autoplay muted loop controls>
                       <source src="videos/meta/planning.mp4" type="video/mp4">
                   </video>
                </p>
                <h3>
                    Abstract
                </h3>
                <p class="text-justify">
                    Large language models have been demonstrated to perform complex tasks. However, enabling general inference in the real world, e.g. for robotics problems, raises the challenge of grounding, i.e., incorporating real-world sensor modalities as input to language models. We propose embodied language models to establish the link between words and percepts by directly incorporating continuous inputs from sensor modalities. Input to our embodied language model are multi-modal sentences that interleave, visual, continuous state estimation, and textual input encodings that are trained end-to-end, in conjunction with a pre-trained large language model, for multiple embodied tasks, including sequential robotic manipulation planning, visual question answering, and captioning.
                    Our evaluations show that a single large embodied multimodal PaLM-E model can address a variety of embodied reasoning tasks, from a variety of observation modalities, on multiple embodiments, and further, exhibits positive transfer: the model benefits from diverse joint training across internet-scale language, vision, and visual-language domains.
                </p>
            </div>
        </div>

        <div class="row">
            <div class="col-md-8 col-md-offset-2">
                <h3>
                    Approach
                </h3>
                <p style="text-align:center;">
                    <img src="img/approach.png" class="img-responsive">
                </p>

                <p class="text-justify">
                    The main architectural idea of PaLM-E is to inject continuous, embodied observations such as images, 
                    state estimates, or other sensor modalities into the language embedding space of a pre-trained language model. 
                    This is realized by encoding the continuous observations into a sequence of vectors with the same dimension 
                    as the embedding space of the language tokens. The continuous information is hence injected into the language model 
                    in an analogous way to language tokens. PaLM-E is a decoder-only LLM that generates textual completions 
                    autoregressively given a prefix or prompt. We call our model PaLM-<b>E</b>, since we use <a href="https://ai.googleblog.com/2022/04/pathways-language-model-palm-scaling-to.html">PaLM (Chowdhery et al., 2022)</a> as 
                    the pre-trained language model, and make it <b>E</b>mbodied.
                </p>    
            </div>
        </div>




    <div class="row">
            <div class="col-md-8 col-md-offset-2">
                <h3>
                    Results
                </h3>
		<p class="text-justify">
        
		We show a few example videos showing how PaLM-E can be used to plan and execute long horizon tasks on two different real embodiments. Please note, that all of these results were obtained using the same model trained on all data.
		In the first video above the abstract, we execute a long-horizon instruction "bring me the rice chips from the drawer" that includes multiple planning steps as well as incorporating visual feedback from the robot's camera. <br><br>
		Below, we show another example on the same robot where the instruction is "bring me a green star". Green star is an object that this robot wasn't directly exposed to.
		</p>

        <p style="test-align:center;">
					<video id="v0" width="100%" playsinline muted loop controls>
                       <source src="videos/meta/green_star.mp4" type="video/mp4">
                   </video>		
        </p>
        <p class="text-justify">

         In the next example, we show the exact same PaLM-E model weights on another robot embodiment. In this case our model is able to successfully plan a long-horizon task "sort blocks by colors into different corners" .
		</p>
        <p style="test-align:center;">
					<video id="v0" width="100%" playsinline muted loop controls>
                       <source src="videos/xarm/sort_by_color_iphone.mp4" type="video/mp4">
                   </video>		
        </p>
        
        Here is another example of this tasks that involves planning over multiple stages and incorporating visual feedback over long time horizons.
		</p>
        <p style="test-align:center;">
					<video id="v0" width="100%" playsinline muted loop controls>
                       <source src="videos/xarm/sort_colors_into_corners_long.mp4" type="video/mp4">
                   </video>		
        </p>
        
        Below, we demonstate another example of long-horizon pushing tasks on this robot. The first instruction is "move remaining blocks to the group".  PaLM-E sequences step-by-step commands to the low-level policy such as "move the yellow hexagon to the green start", and "move the blue triangle to the group".
		</p>
        <p style="test-align:center;">
					<video id="v0" width="100%" playsinline muted loop controls>
                       <source src="videos/xarm/remaining_blocks_to_group.mp4" type="video/mp4">
                   </video>		
        </p>
        
        
        Whereas in this video the instruction is: "push the ocean colored blocks together".
		</p>
        <p style="test-align:center;">
					<video id="v0" width="100%" playsinline muted loop controls>
                       <source src="videos/xarm/ocean_colored_blocks_together.mp4" type="video/mp4">
                   </video>		
        </p>
        
        Next, we demonstrate two examples of generalization. In the case below the instruction is "push red blocks to the coffee cup". The dataset contains only three demonstrations with the coffee cup in them, and none of them included red blocks.
		</p>
        <p style="test-align:center;">
					<video id="v0" width="100%" playsinline muted loop controls>
                       <source src="videos/xarm/red_blocks_to_coffee.mp4" type="video/mp4">
                   </video>		
        </p>
        
        Lastly, we show another generalization example, where the instruction is "push green blocks to the turtle". The robot is able to successfully execute this task even though it has never seen the turtle before.
		</p>
        <p style="test-align:center;">
					<video id="v0" width="100%" playsinline muted loop controls>
                       <source src="videos/xarm/green_blocks_to_turtle.mp4" type="video/mp4">
                   </video>		
        </p>
	    </div>
        </div>

        <div class="row">
            <div class="col-md-8 col-md-offset-2">
                <h3>
                    Demo
                </h3>
                <div class="row">
                    <div class="col-md-2">
                      <img src="img/demo_img/bike.png" width="100%" alt="Given <b>img</b>. Q: Can I go down this street on a bicycle, yes or no? A: Let's think step by step. [sep] 1. do not enter. 2. except bicycles. 3. do not enter except bicycles. 4. yes." onclick="myFunction(this);">
                    </div>
                    <div class="col-md-2">
                      <img src="img/demo_img/donut.png" width="100%" alt="Given <b>img</b>. Q: What flavor is the pink doughnut on the right? A:[sep] strawberry.. Q: What flavor is the doughnut on the left? A blueberry. " onclick="myFunction(this);">
                    </div>
                    <div class="col-md-2">
                      <img src="img/demo_img/darpa-robot.jpg" width="100%" alt="Given <b>img</b>. Q: What will the robot do next? A: [sep] fall." onclick="myFunction(this);">
                    </div>
                    <div class="col-md-2">
                      <img src="img/demo_img/pokemon.png" width="100%" alt=" Q: Between <b>img</b> and <b>img</b>, who would win? A: Let's think step by step.[sep]1. Blastoise is a water type pokemon. 2. Charizard is a fire type pokemon. 3. Water is stronger than fire. 4. Blastoise is stronger than Charizard. 5. Blastoise wins. 6. The end." onclick="myFunction(this);">
                    </div>
                    <div class="col-md-2">
                      <img src="img/demo_img/compare.png" width="100%" alt="Photo 1: <b>img</b>.  Photo 2: <b>img</b>.  Q: What is in Photo 1 but NOT in Photo 2? A: Let's think step by step and explain.[sep] Photo 1 has sunglasses on top of folded clothes. Photo 2 does not have sunglasses on top of folded clothes. Therefore, sunglasses are in Photo 1 but not in Photo 2." onclick="myFunction(this);">
                    </div>
                    <div class="col-md-2">
                      <img src="img/demo_img/donkey.jpg" width="100%" alt="I see (image omitted). Description: A cow sitting on a rug. Joke: Why is the cow sitting on a rug? Because it wants to be a cow-ch!I see <b>img</b>.[sep] Description: a donkey is carrying a dog, cat, and rooster. Joke: what do you call a donkey with a rooster on his back. A rooster booster. "
                       onclick="myFunction(this);">
                    </div>
                </div>
                <p></p>

                <div class="row">
                    <div class="col-md-6">
                        <img id="expandedImg" style="width:100%">
                    </div>
                    <div class="col-md-6">
                    <div id="imgtext"></div>
                    <div><p style="test-align:center" id="answer"></p></div>
                    </div>
                    
                </div>
        
            </div>
        </div>

        <div class="row">
            <div class="col-md-8 col-md-offset-2">
                <h3>
                    Citation 
                </h3>
                <div class="form-group col-md-10 col-md-offset-1">
                    <textarea id="bibtex" class="form-control" readonly>
@inproceedings{driess2023palme,
    title={PaLM-E: An Embodied Multimodal Language Model},
    author={},
    booktitle={arXiv preprint arXiv:2302.11111},
    year={2023}
}</textarea>
                </div>
            </div>
             
        </div>


        <div class="row">
            <div class="col-md-8 col-md-offset-2">
                <h3>
                    Acknowledgements
                </h3>
                <p class="text-justify">
                    We would like to acknowledge the greater teams at Robotics at Google for their feedback and contributions.
                </p>
            </div>
        </div>





    </div>
</body>
</html>
